#include <algorithm>
#include <cfloat>
#include <vector>
#include <thrust/host_vector.h>
#include <thrust/device_vector.h>
#include <thrust/sort.h>
#include <thrust/unique.h>
#include <iterator>
#include <stdio.h>

#include "caffe/layer.hpp"
#include "caffe/util/math_functions.hpp"
#include "caffe/vision_layers.hpp"

namespace caffe {

template <typename Dtype>
__global__ void SoftmaxLossExpectationForwardGPU(const int nthreads,
          const Dtype* prob_data, const Dtype* label, Dtype* loss,
          const int num, const int dim, const int spatial_dim,
          const bool has_ignore_label_, const int ignore_label_, 
					const bool ignore_objectness_, const bool ignore_location_, const bool ignore_constraint_,
          Dtype* counts) {
  CUDA_KERNEL_LOOP(index, nthreads) {
    //const int n = index / spatial_dim;
    const int s = index % spatial_dim;
    const int label_value = static_cast<int>(label[s]);

		int len = 0;
		double sum_present_classes = 0;
		while (true) { // Gets all the unique numbers out of the 3rd channel
			int class_ = static_cast<int>(label[2*spatial_dim + len]);
			if (class_ == 0) break;
			double Sc = max(prob_data[class_ * spatial_dim + s], Dtype(FLT_MIN)); // P(class=c)  
      sum_present_classes += Sc;
      len++;
		}

		// No clicks means it's strongly supervised -- every pixel should be labelled as the background
		/*if (len == 0 && !(ignore_objectness_ && ignore_constraint_)) { 
			loss[index] = -log(max(prob_data[0 * spatial_dim + s],
                      Dtype(FLT_MIN))); 
			counts[index] = 1;
		}*/
		// If there are some clicks, then either:
    // (a) Ignore location (i.e. all pixels are unsupervised), or
    // (b) We do not know the target label because we don't have a user click
	  /*else*/ if ((ignore_location_ || has_ignore_label_ && label_value == ignore_label_)) {
			const int objectness = static_cast<int>(label[1 * spatial_dim + s]); // Value between 0 and 255
    	const double S0 = max(prob_data[0 * spatial_dim + s], Dtype(FLT_MIN)); // P(class=0) in our model
    	const double P = 1.0 - ((double)objectness / 255.0); // P(class=0) acording to prior
			const double a0 = P;
			const double a_c = (1-P)/len;

			if (ignore_objectness_ && ignore_constraint_) { // No objectness, no constraint
				loss[index] = 0; // Basically SoftmaxWithLoss	
				counts[index] = 0;
			} else if (ignore_objectness_ && !ignore_constraint_) { // No objectness, constraint
				loss[index] = -log( S0 + sum_present_classes );
				counts[index] = 1;
			} else if (!ignore_objectness_ && ignore_constraint_) { // Objectness, no constraint
				loss[index] = -log( (P*S0) + (1-P)*(1-S0)); 
				counts[index] = 1;
			} else if (!ignore_objectness_ && !ignore_constraint_) { // Objectness, constraint 
				if (len == 0) {
					loss[index] = 0;
					counts[index] = 0;
				} else {
					loss[index] = -log( (a0*S0) + (a_c * sum_present_classes));
					counts[index] = 1;
				}
			}

		// Supervised; we do know the target label: Loss(class=t) = -log(S_t)
    } else {
      loss[index] = -log(max(prob_data[label_value * spatial_dim + s],
                      Dtype(FLT_MIN)));
			counts[index] = 1;
    }
  }
}

template <typename Dtype>
void SoftmaxWithLossExpectationLayer<Dtype>::Forward_gpu(
    const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) {
  softmax_layer_->Forward(softmax_bottom_vec_, softmax_top_vec_);
  const Dtype* prob_data = prob_.gpu_data();
  const Dtype* label = bottom[1]->gpu_data();
  const int dim = prob_.count() / outer_num_;
  CHECK_EQ(outer_num_ * inner_num_ * 3, bottom[1]->count(0))
      << "Number of labels must match the number of predictions because there are three channels,"
      << "one for gt labels per pixel and one for objectness labels per pixel and one for unique classes; "
      << "e.g., if softmax axis == 1 and prediction shape is (N, C, H, W), "
      << "label count (number of labels) must be 3*N*H*W, "
      << "with integer values in {0, 1, ..., C-1}.";

  const int nthreads = outer_num_ * inner_num_;
  // Since this memory is not used for anything until it is overwritten
  // on the backward pass, we use it here to avoid having to allocate new GPU
  // memory to accumulate intermediate results in the kernel.
  Dtype* loss_data = bottom[0]->mutable_gpu_diff();
  // Similarly, this memory is never used elsewhere, and thus we can use it
  // to avoid having to allocate additional GPU memory.
  Dtype* counts = prob_.mutable_gpu_diff();

  // NOLINT_NEXT_LINE(whitespace/operators)
  SoftmaxLossExpectationForwardGPU<Dtype><<<CAFFE_GET_BLOCKS(nthreads),
      CAFFE_CUDA_NUM_THREADS>>>(nthreads, prob_data, label, loss_data,
      outer_num_, dim, inner_num_, has_ignore_label_, ignore_label_, ignore_objectness_, ignore_location_, ignore_constraint_, counts);
  Dtype loss;
  caffe_gpu_asum(nthreads, loss_data, &loss);
  if (normalize_) {
    Dtype count;
    caffe_gpu_asum(nthreads, counts, &count);
    loss /= count;
  } else {
    loss /= outer_num_;
  }
  top[0]->mutable_cpu_data()[0] = loss;
  if (top.size() == 2) {
    top[1]->ShareData(prob_);
  }
}

template <typename Dtype>
__global__ void SoftmaxLossExpectationBackwardGPU(const int nthreads, const Dtype* prob_data, const Dtype* top,
          const Dtype* label, Dtype* bottom_diff, const int num, const int dim,
          const int spatial_dim, const bool has_ignore_label_,
          const int ignore_label_, const bool ignore_objectness_, const bool ignore_location_, const bool ignore_constraint_, Dtype* counts) {
  const int channels = dim / spatial_dim;

  CUDA_KERNEL_LOOP(index, nthreads) {
    //const int n = index / spatial_dim;
    const int s = index % spatial_dim;
    const int label_value = static_cast<int>(label[s]);

		double sum_present_classes = 0;
		int len = 0; 
		// Get sum of present, non-bg classes
		while (true) { 
      int class_ = static_cast<int>(label[2*spatial_dim + len]);
      if (class_ == 0) break; // We've reached the end of the present classes
			double Sc = max(prob_data[class_ * spatial_dim + s], Dtype(FLT_MIN)); // P(class=class_)
      len++;
			sum_present_classes += Sc;
    }

		// No clicks means it's strongly supervised -- every pixel should be labelled as the background
    /*if (len == 0 && !(ignore_objectness_ && ignore_constraint_)) {
			bottom_diff[0 * spatial_dim + s] -= 1;
			counts[index] = 1;
    }*/
    // If there are some clicks, then either:
		// (a) Ignore location (i.e. all pixels are unsupervised), or
    // (b) We do not know the target label because we don't have a user click 
		/*else*/ if (ignore_location_ || (has_ignore_label_ && label_value == ignore_label_)) {
			const int objectness = static_cast<int>(label[spatial_dim + s]); // Value between 0 and 255
      const double S0 = max(prob_data[0 * spatial_dim + s], Dtype(FLT_MIN)); // P(bg) in our model
      const double P = 1.0 - ((double)objectness / 255.0); // P(bg) acording to prior

			// Iterates over all channels 
			for (int i = 0; i < channels; i++) {
				// Checks if image contains this particular class 	
				bool img_contains_class = false;
    		while (true) {
      		int class_ = static_cast<int>(label[2*spatial_dim + len]);
      		if (class_ == 0) break; // We've reached the end of the present classes
      		if (class_ == i) img_contains_class = true;
    		}

				if (ignore_objectness_ && ignore_constraint_) { // No objectness, no constraint
					bottom_diff[i * spatial_dim + s] = 0;
					counts[index] = 0;
				} else if (!ignore_objectness_ && ignore_constraint_) { // Objectness, no constraint 
					double a_i;
					if (i == 0) a_i = P; // background
					else a_i = 1-P; // not background
					double denom = P*S0 + (1-P)*(1-S0); 
					bottom_diff[i * spatial_dim + s] *= (1 - (a_i / denom));
					counts[index] = 1; 
				} else if (ignore_objectness_ && !ignore_constraint_) { // No objectness, constraint
					if (img_contains_class || i == 0) {
						double denom = S0 + sum_present_classes;
						bottom_diff[i * spatial_dim + s] *= (1 - (1 / denom)); 
					}	// Otherwise grad is just S_i
					counts[index] = 1;
				} else if (!ignore_objectness_ && !ignore_constraint_) { // Objectness, constraint 
					if (len == 0) {
						bottom_diff[i * spatial_dim + s] = 0;
					}
					else if (img_contains_class || i == 0) {
						double a_i;
						if (i == 0) a_i = P; // background
						else a_i = (1-P)/len;
						double denom = P*S0 + ((1-P)/len)*sum_present_classes;
						bottom_diff[i * spatial_dim + s] *= (1 - (a_i / denom));
					} // Otherwise grad is just S_i
					counts[index] = 1;
				}
			}
    } else { // If we have a user click, the gradient calculation stays the same
      bottom_diff[label_value * spatial_dim + s] -= 1;
			counts[index] = 1;
    }
  }
}

template <typename Dtype>
void SoftmaxWithLossExpectationLayer<Dtype>::Backward_gpu(const vector<Blob<Dtype>*>& top,
    const vector<bool>& propagate_down, const vector<Blob<Dtype>*>& bottom) {
  if (propagate_down[1]) {
    LOG(FATAL) << this->type()
               << " Layer cannot backpropagate to label inputs.";
  }
  if (propagate_down[0]) {
    Dtype* bottom_diff = bottom[0]->mutable_gpu_diff();
    const Dtype* prob_data = prob_.gpu_data();
    const Dtype* top_data = top[0]->gpu_data();
    caffe_gpu_memcpy(prob_.count() * sizeof(Dtype), prob_data, bottom_diff);
    const Dtype* label = bottom[1]->gpu_data();
    const int dim = prob_.count() / outer_num_;
    const int nthreads = outer_num_ * inner_num_;
    // Since this memory is never used for anything else,
    // we use to to avoid allocating new GPU memory.
    Dtype* counts = prob_.mutable_gpu_diff();

    // NOLINT_NEXT_LINE(whitespace/operators)
    SoftmaxLossExpectationBackwardGPU<Dtype><<<CAFFE_GET_BLOCKS(nthreads),
        CAFFE_CUDA_NUM_THREADS>>>(nthreads, prob_data, top_data, label, bottom_diff,
        outer_num_, dim, inner_num_, has_ignore_label_, ignore_label_, ignore_objectness_, ignore_location_, ignore_constraint_, counts);
    const Dtype loss_weight = top[0]->cpu_diff()[0];
    if (normalize_) {
      Dtype count;
      caffe_gpu_asum(nthreads, counts, &count);
      caffe_gpu_scal(prob_.count(), loss_weight / count, bottom_diff);
    } else {
      caffe_gpu_scal(prob_.count(), loss_weight / outer_num_, bottom_diff);
    }
  }
}

INSTANTIATE_LAYER_GPU_FUNCS(SoftmaxWithLossExpectationLayer);

}  // namespace caffe
